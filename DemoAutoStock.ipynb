{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXJN_7FSmJx1",
        "outputId": "c21659bb-303f-43b1-a32c-07fbfda8a7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "\n",
            "--- Performing minimal data cleaning on the provided file ---\n",
            "Removed 132220 rows with missing values.\n",
            "Total rows after cleaning: 397884\n",
            "\n",
            "--- Aggregating data for time-series forecasting ---\n",
            "Time-series DataFrame created successfully.\n",
            "StockCode            10002  10080  10120  10123C  10124A  10124G  10125  \\\n",
            "InvoiceDate                                                               \n",
            "2010-12-01 08:26:00    0.0    0.0    0.0     0.0     0.0     0.0    0.0   \n",
            "2010-12-01 08:28:00    0.0    0.0    0.0     0.0     0.0     0.0    0.0   \n",
            "2010-12-01 08:34:00    0.0    0.0    0.0     0.0     0.0     0.0    0.0   \n",
            "2010-12-01 08:35:00    0.0    0.0    0.0     0.0     0.0     0.0    0.0   \n",
            "2010-12-01 08:45:00   48.0    0.0    0.0     0.0     0.0     0.0    0.0   \n",
            "\n",
            "StockCode            10133  10135  11001  ...  90214V  90214W  90214Y  90214Z  \\\n",
            "InvoiceDate                               ...                                   \n",
            "2010-12-01 08:26:00    0.0    0.0    0.0  ...     0.0     0.0     0.0     0.0   \n",
            "2010-12-01 08:28:00    0.0    0.0    0.0  ...     0.0     0.0     0.0     0.0   \n",
            "2010-12-01 08:34:00    0.0    0.0    0.0  ...     0.0     0.0     0.0     0.0   \n",
            "2010-12-01 08:35:00    0.0    0.0    0.0  ...     0.0     0.0     0.0     0.0   \n",
            "2010-12-01 08:45:00    0.0    0.0    0.0  ...     0.0     0.0     0.0     0.0   \n",
            "\n",
            "StockCode            BANK CHARGES   C2  DOT    M  PADS  POST  \n",
            "InvoiceDate                                                   \n",
            "2010-12-01 08:26:00           0.0  0.0  0.0  0.0   0.0   0.0  \n",
            "2010-12-01 08:28:00           0.0  0.0  0.0  0.0   0.0   0.0  \n",
            "2010-12-01 08:34:00           0.0  0.0  0.0  0.0   0.0   0.0  \n",
            "2010-12-01 08:35:00           0.0  0.0  0.0  0.0   0.0   0.0  \n",
            "2010-12-01 08:45:00           0.0  0.0  0.0  0.0   0.0   3.0  \n",
            "\n",
            "[5 rows x 3665 columns]\n",
            "\n",
            "--- Performing Market Basket Analysis using Apriori ---\n",
            "\n",
            "Association rules generated successfully.\n",
            "--- Top 5 Association Rules (by lift) ---\n",
            "                                          antecedents  \\\n",
            "73                  (GREEN REGENCY TEACUP AND SAUCER)   \n",
            "72  (ROSES REGENCY TEACUP AND SAUCER , PINK REGENC...   \n",
            "70  (GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...   \n",
            "75                   (PINK REGENCY TEACUP AND SAUCER)   \n",
            "5                    (PINK REGENCY TEACUP AND SAUCER)   \n",
            "\n",
            "                                          consequents  antecedent support  \\\n",
            "73  (ROSES REGENCY TEACUP AND SAUCER , PINK REGENC...            0.036763   \n",
            "72                  (GREEN REGENCY TEACUP AND SAUCER)            0.023007   \n",
            "70                   (PINK REGENCY TEACUP AND SAUCER)            0.028594   \n",
            "75  (GREEN REGENCY TEACUP AND SAUCER, ROSES REGENC...            0.029615   \n",
            "5                   (GREEN REGENCY TEACUP AND SAUCER)            0.029615   \n",
            "\n",
            "    consequent support   support  confidence       lift  representativity  \\\n",
            "73            0.023007  0.020484    0.557190  24.218105               1.0   \n",
            "72            0.036763  0.020484    0.890339  24.218105               1.0   \n",
            "70            0.029615  0.020484    0.716387  24.190034               1.0   \n",
            "75            0.028594  0.020484    0.691684  24.190034               1.0   \n",
            "5             0.036763  0.024269    0.819473  22.290459               1.0   \n",
            "\n",
            "    leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
            "73  0.019638    2.206345       0.995299  0.521407   0.546762    0.723764  \n",
            "72  0.019638    8.783801       0.981285  0.521407   0.886154    0.723764  \n",
            "70  0.019637    3.421506       0.986879  0.542994   0.707731    0.704035  \n",
            "75  0.019637    3.150680       0.987918  0.542994   0.682608    0.704035  \n",
            "5   0.023180    5.335681       0.984287  0.576320   0.812583    0.739802  \n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "def load_and_clean_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the OnlineRetail.csv dataset and performs initial cleaning.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(filepath, encoding='latin1')\n",
        "        print(\"Data loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {filepath} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Step 1: Data Cleaning (Simplified for pre-cleaned file) ---\n",
        "    print(\"\\n--- Performing minimal data cleaning on the provided file ---\")\n",
        "\n",
        "    # Convert 'InvoiceDate' to datetime objects for time-series analysis\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "    # Drop any rows with remaining missing values, just to be safe\n",
        "    initial_rows = len(df)\n",
        "    df.dropna(inplace=True)\n",
        "    print(f\"Removed {initial_rows - len(df)} rows with missing values.\")\n",
        "\n",
        "    # Ensure StockCode is treated as a string\n",
        "    df['StockCode'] = df['StockCode'].astype(str)\n",
        "\n",
        "    print(f\"Total rows after cleaning: {len(df)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def perform_data_aggregation_and_mining(df):\n",
        "    \"\"\"\n",
        "    Performs data aggregation for time-series analysis and market basket analysis.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The cleaned DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the time-series DataFrame and the association rules DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Step 2: Feature Engineering & Aggregation for Time-Series ---\n",
        "    print(\"\\n--- Aggregating data for time-series forecasting ---\")\n",
        "\n",
        "    # Group by date and product to get daily sales\n",
        "    daily_sales = df.groupby(['InvoiceDate', 'StockCode'])['Quantity'].sum().reset_index()\n",
        "\n",
        "    # Pivot the table to have products as columns and dates as rows\n",
        "    time_series_df = daily_sales.pivot_table(\n",
        "        index='InvoiceDate', columns='StockCode', values='Quantity'\n",
        "    ).fillna(0)\n",
        "\n",
        "    print(\"Time-series DataFrame created successfully.\")\n",
        "    print(time_series_df.head())\n",
        "\n",
        "    # --- Step 3: Association Rule Mining (Market Basket Analysis) ---\n",
        "    print(\"\\n--- Performing Market Basket Analysis using Apriori ---\")\n",
        "\n",
        "    # To avoid memory issues, we will focus the market basket analysis on a single country\n",
        "    basket_df = df[df['Country'] == 'United Kingdom']\n",
        "\n",
        "    # Create a new DataFrame for market basket analysis using a more stable method.\n",
        "    basket_sets = basket_df.pivot_table(index='InvoiceNo', columns='Description', values='Quantity', aggfunc='sum').fillna(0).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "    # Apply the Apriori algorithm to find frequent itemsets\n",
        "    frequent_itemsets = apriori(basket_sets, min_support=0.02, use_colnames=True)\n",
        "\n",
        "    # Generate association rules with confidence and lift metrics\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "    print(\"\\nAssociation rules generated successfully.\")\n",
        "    print(\"--- Top 5 Association Rules (by lift) ---\")\n",
        "    print(rules.sort_values('lift', ascending=False).head())\n",
        "\n",
        "    return time_series_df, rules\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    file_path = 'Cleaned_OnlineRetail.csv'\n",
        "    cleaned_df = load_and_clean_data(file_path)\n",
        "\n",
        "    if cleaned_df is not None:\n",
        "        time_series_df, rules_df = perform_data_aggregation_and_mining(cleaned_df)\n",
        "\n",
        "        # Now you can use time_series_df for Phase 2 (AI Model Training)\n",
        "        # and rules_df for business insights and the agent's logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy tensorflow scikit-learn statsmodels mlxtend keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12yRMh3IoAu-",
        "outputId": "ce8e6e7d-88fa-4b34-fb0d-428c3c681b3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def train_model(time_series_df):\n",
        "    \"\"\"\n",
        "    Trains a hybrid ARIMA-LSTM model for demand forecasting.\n",
        "\n",
        "    Args:\n",
        "        time_series_df (pd.DataFrame): The time-series data from data_preparation.py.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Select a single product for a proof of concept ---\n",
        "    # We will pick '85123A' as it is a top-selling product\n",
        "    if '85123A' not in time_series_df.columns:\n",
        "        print(\"Product '85123A' not found in DataFrame. Please select a valid product code.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    product_data = time_series_df['85123A'].values.reshape(-1, 1)\n",
        "\n",
        "    # Check for empty or all-zero data\n",
        "    if np.sum(product_data) == 0:\n",
        "        print(\"Selected product has no sales data. Cannot train model.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # --- Step 2: Data Preprocessing for LSTM ---\n",
        "    # Scale the data to be between 0 and 1, which LSTMs prefer\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(product_data)\n",
        "\n",
        "    # Create sequences for the LSTM model\n",
        "    def create_sequences(data, seq_length):\n",
        "        xs, ys = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            x = data[i:(i + seq_length)]\n",
        "            y = data[i + seq_length]\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)\n",
        "\n",
        "    # Use a sequence length of 7 (for a week)\n",
        "    sequence_length = 7\n",
        "    if len(scaled_data) < sequence_length + 1:\n",
        "        print(\"Not enough data to create sequences. Please use a longer time series.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    X, y = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # --- Step 3: Implement the Hybrid ARIMA-LSTM Model ---\n",
        "    # Part A: ARIMA model to capture linear trends\n",
        "    try:\n",
        "        arima_model = ARIMA(product_data[:train_size].flatten(), order=(1, 1, 1))\n",
        "        arima_model_fit = arima_model.fit()\n",
        "        arima_residuals = arima_model_fit.resid\n",
        "    except Exception as e:\n",
        "        print(f\"ARIMA model failed to fit: {e}. Skipping ARIMA step.\")\n",
        "        # If ARIMA fails, we can still try to train the LSTM on the raw data\n",
        "        arima_residuals = product_data[:train_size].flatten()\n",
        "\n",
        "    # Part B: LSTM model to capture non-linear patterns from residuals\n",
        "    # Scale the residuals\n",
        "    residual_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Ensure there are enough residuals to train the LSTM model\n",
        "    if len(arima_residuals) < sequence_length + 1:\n",
        "        print(\"Not enough residuals to create LSTM sequences.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    scaled_residuals = residual_scaler.fit_transform(arima_residuals.reshape(-1, 1))\n",
        "\n",
        "    # Create sequences for the LSTM from the scaled residuals\n",
        "    X_res, y_res = create_sequences(scaled_residuals, sequence_length)\n",
        "\n",
        "    if len(X_res) < 1:\n",
        "        print(\"Not enough data after creating sequences from residuals.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    X_res_train = X_res\n",
        "    y_res_train = y_res\n",
        "\n",
        "    # Reshape for LSTM\n",
        "    X_res_train = X_res_train.reshape((X_res_train.shape[0], X_res_train.shape[1], 1))\n",
        "\n",
        "    # Build the LSTM model\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(50, activation='relu', input_shape=(X_res_train.shape[1], X_res_train.shape[2])))\n",
        "    lstm_model.add(Dense(1))\n",
        "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the LSTM model\n",
        "    print(\"\\nTraining LSTM model...\")\n",
        "    lstm_model.fit(X_res_train, y_res_train, epochs=20, verbose=1)\n",
        "\n",
        "    print(\"\\nModel training complete.\")\n",
        "\n",
        "    # --- Step 4: Model Evaluation (for a single step prediction) ---\n",
        "    print(\"\\n--- Evaluating Model Performance ---\")\n",
        "\n",
        "    # Make a prediction on the test data\n",
        "    # Predict residuals using the LSTM model\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "    lstm_predictions_scaled = lstm_model.predict(X_test_reshaped)\n",
        "\n",
        "    # Inverse transform the scaled predictions to original scale\n",
        "    lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
        "\n",
        "    # Make a prediction with ARIMA on the test data\n",
        "    try:\n",
        "        arima_forecast = arima_model_fit.forecast(steps=len(X_test)).to_numpy().reshape(-1, 1)\n",
        "    except:\n",
        "        arima_forecast = np.zeros_like(lstm_predictions) # Fallback if ARIMA failed\n",
        "\n",
        "    # Combine the predictions\n",
        "    final_predictions = arima_forecast[:len(lstm_predictions)] + lstm_predictions\n",
        "\n",
        "    # Inverse transform the original test data\n",
        "    actual_values = scaler.inverse_transform(y_test[:len(final_predictions)])\n",
        "\n",
        "    # Calculate Mean Absolute Error (MAE)\n",
        "    mae = np.mean(np.abs(final_predictions - actual_values))\n",
        "    print(f\"Mean Absolute Error (MAE) on test data: {mae:.2f}\")\n",
        "\n",
        "    return arima_model_fit, lstm_model, scaler, residual_scaler\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # You must run data_preparation.py first to get this DataFrame\n",
        "    # For now, we will create a dummy DataFrame to allow the code to be run independently\n",
        "    # In your final project, you would import the actual DataFrame\n",
        "\n",
        "    # Create a dummy DataFrame to represent daily sales for a single product\n",
        "    dummy_dates = pd.date_range(start='2010-12-01', end='2011-12-09', freq='D')\n",
        "    dummy_data = np.random.randint(0, 100, size=(len(dummy_dates), 1))\n",
        "\n",
        "    dummy_df = pd.DataFrame(data=dummy_data, index=dummy_dates, columns=['85123A'])\n",
        "\n",
        "    arima_model, lstm_model, main_scaler, res_scaler = train_model(dummy_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnrXb5IRn1a4",
        "outputId": "d7bb4b8c-5942-417f-f6ce-60c62614d906"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM model...\n",
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 326ms/step - loss: 0.2851\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1923 \n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1224 \n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0799 \n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0766 \n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0742 \n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0785 \n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0748 \n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0755 \n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0731 \n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0743 \n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0759 \n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0792 \n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0771 \n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0680 \n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0697 \n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0685 \n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0711 \n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0755 \n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0686 \n",
            "\n",
            "Model training complete.\n",
            "\n",
            "--- Evaluating Model Performance ---\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step\n",
            "Mean Absolute Error (MAE) on test data: 27.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We need to recreate the models and scalers from the previous step for this file to be runnable on its own.\n",
        "# In a real project, these trained models would be saved and loaded from files.\n",
        "\n",
        "def create_dummy_models_and_data():\n",
        "    \"\"\"\n",
        "    Creates dummy data and trains simplified dummy models to make the\n",
        "    agent_creation.py file runnable on its own.\n",
        "    In a real project, this would be replaced with loading your trained models.\n",
        "    \"\"\"\n",
        "    print(\"--- Creating dummy models and data for agent simulation ---\")\n",
        "\n",
        "    # Dummy data\n",
        "    dummy_dates = pd.date_range(start='2010-12-01', end='2011-12-09', freq='D')\n",
        "    dummy_data = np.random.randint(0, 100, size=(len(dummy_dates), 1))\n",
        "\n",
        "    # Dummy ARIMA model\n",
        "    arima_model = ARIMA(dummy_data[:200].flatten(), order=(1, 1, 1))\n",
        "    arima_model_fit = arima_model.fit()\n",
        "\n",
        "    # Dummy LSTM model\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(50, activation='relu', input_shape=(7, 1)))\n",
        "    lstm_model.add(Dense(1))\n",
        "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Dummy scalers\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler.fit(dummy_data)\n",
        "\n",
        "    return arima_model_fit, lstm_model, scaler\n",
        "\n",
        "def forecast_demand(arima_model_fit, lstm_model, scaler, historical_data, forecast_horizon=7):\n",
        "    \"\"\"\n",
        "    Generates a demand forecast for the next 'forecast_horizon' days.\n",
        "\n",
        "    Args:\n",
        "        arima_model_fit: The fitted ARIMA model object.\n",
        "        lstm_model: The trained LSTM model object.\n",
        "        scaler: The MinMaxScaler used for the data.\n",
        "        historical_data (np.array): The most recent historical data for the model's sequence length.\n",
        "        forecast_horizon (int): The number of days to forecast into the future.\n",
        "\n",
        "    Returns:\n",
        "        np.array: An array of forecasted demand for the next 'forecast_horizon' days.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Forecasting demand for the next {forecast_horizon} days ---\")\n",
        "\n",
        "    forecasts = []\n",
        "    current_data = historical_data.copy()\n",
        "\n",
        "    for _ in range(forecast_horizon):\n",
        "        # 1. Forecast with ARIMA\n",
        "        # Use the most recent data to forecast the next step\n",
        "        arima_forecast = arima_model_fit.forecast(steps=1)\n",
        "\n",
        "        # 2. Scale the current data for LSTM input\n",
        "        current_data_scaled = scaler.transform(current_data.reshape(-1, 1))\n",
        "\n",
        "        # 3. Predict the non-linear component (residual) with LSTM\n",
        "        lstm_input = current_data_scaled.reshape(1, current_data_scaled.shape[0], 1)\n",
        "        lstm_residual_scaled = lstm_model.predict(lstm_input, verbose=0)\n",
        "        lstm_residual = scaler.inverse_transform(lstm_residual_scaled)[0, 0]\n",
        "\n",
        "        # 4. Combine the forecasts\n",
        "        combined_forecast = arima_forecast + lstm_residual\n",
        "\n",
        "        forecasts.append(combined_forecast)\n",
        "\n",
        "        # 5. Update the historical data for the next prediction step\n",
        "        current_data = np.append(current_data[1:], combined_forecast)\n",
        "\n",
        "\n",
        "    return np.array(forecasts).flatten()\n",
        "\n",
        "def calculate_restock_order(forecast, current_stock, lead_time=3, safety_stock_multiplier=1.2):\n",
        "    \"\"\"\n",
        "    Calculates the optimal restocking order quantity based on forecast and current inventory.\n",
        "\n",
        "    Args:\n",
        "        forecast (np.array): Array of forecasted demand.\n",
        "        current_stock (int): The current number of items in stock.\n",
        "        lead_time (int): Time in days to receive a new order.\n",
        "        safety_stock_multiplier (float): Multiplier for safety stock.\n",
        "\n",
        "    Returns:\n",
        "        int: The recommended restocking order quantity.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Calculating restock order ---\")\n",
        "\n",
        "    # Calculate demand over the lead time period\n",
        "    demand_during_lead_time = np.sum(forecast[:lead_time])\n",
        "\n",
        "    # Calculate safety stock\n",
        "    average_daily_demand = np.mean(forecast)\n",
        "    safety_stock = int(average_daily_demand * safety_stock_multiplier)\n",
        "\n",
        "    # Calculate optimal restock point\n",
        "    restock_point = demand_during_lead_time + safety_stock\n",
        "\n",
        "    # Determine the order quantity\n",
        "    if current_stock < restock_point:\n",
        "        order_quantity = restock_point - current_stock\n",
        "        print(f\"Current Stock: {current_stock}, Restock Point: {restock_point}\")\n",
        "        print(f\"Demand during lead time: {demand_during_lead_time}, Safety Stock: {safety_stock}\")\n",
        "        print(f\"Order recommended: {int(order_quantity)} units.\")\n",
        "        return int(order_quantity)\n",
        "    else:\n",
        "        print(f\"Current Stock ({current_stock}) is above the restock point ({restock_point}). No order needed.\")\n",
        "        return 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Step 1: Initialize the models and data for a single product ---\n",
        "    # In a real application, you would load your trained models and the latest data\n",
        "\n",
        "    # Get a dummy model and scaler\n",
        "    arima_model, lstm_model, scaler = create_dummy_models_and_data()\n",
        "\n",
        "    # We need recent historical data to make a new forecast\n",
        "    historical_data = np.random.randint(0, 100, size=(7,))\n",
        "\n",
        "    # --- Step 2: The Agent's Automation Loop ---\n",
        "    # This loop represents the daily or weekly automation\n",
        "\n",
        "    # --- Part A: Forecast future demand ---\n",
        "    forecasted_demand = forecast_demand(\n",
        "        arima_model_fit=arima_model,\n",
        "        lstm_model=lstm_model,\n",
        "        scaler=scaler,\n",
        "        historical_data=historical_data\n",
        "    )\n",
        "\n",
        "    print(\"\\nForecasted demand for the next 7 days:\")\n",
        "    print(np.round(forecasted_demand, 2))\n",
        "\n",
        "    # --- Part B: Calculate the restocking order ---\n",
        "    # Simulate a current stock level from a sensor or database\n",
        "    current_inventory = 50\n",
        "\n",
        "    restock_order = calculate_restock_order(\n",
        "        forecast=forecasted_demand,\n",
        "        current_stock=current_inventory\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFinal recommended order quantity: {restock_order} units.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqkmutMKo4UQ",
        "outputId": "832f978e-3bb3-4aa2-ef6c-0ccc1303b0cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating dummy models and data for agent simulation ---\n",
            "\n",
            "--- Forecasting demand for the next 7 days ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dc30e2bf380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecasted demand for the next 7 days:\n",
            "[51.36 51.43 51.19 51.19 50.9  50.97 51.21]\n",
            "\n",
            "--- Calculating restock order ---\n",
            "Current Stock: 50, Restock Point: 214.98068842957593\n",
            "Demand during lead time: 153.98068842957593, Safety Stock: 61\n",
            "Order recommended: 164 units.\n",
            "\n",
            "Final recommended order quantity: 164 units.\n"
          ]
        }
      ]
    }
  ]
}