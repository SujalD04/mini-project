{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i_m55MpwXCH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "from math import sqrt\n",
        "# ---- USER SETTINGS ----\n",
        "DATA_DIR = \"/content/synthetic_supply_chain_data\"\n",
        "SHIPMENTS_CSV = os.path.join(DATA_DIR, \"shipments.csv\")\n",
        "LANES_CSV = os.path.join(DATA_DIR, \"lanes.csv\")\n",
        "WAREHOUSES_CSV = os.path.join(DATA_DIR, \"warehouses.csv\")\n",
        "TRANSPORTS_CSV = os.path.join(DATA_DIR, \"transports.csv\")\n",
        "OUTPUT_MODEL_PATH = \"/content/compat_model.joblib\"\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "# ------------------------\n",
        "\n",
        "print(\"üöÄ Starting compatibility model training...\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 1Ô∏è‚É£ Load data\n",
        "# --------------------------\n",
        "print(\"üìÇ Loading datasets from:\", DATA_DIR)\n",
        "shipments = pd.read_csv(SHIPMENTS_CSV)\n",
        "lanes = pd.read_csv(LANES_CSV)\n",
        "warehouses = pd.read_csv(WAREHOUSES_CSV)\n",
        "transports = pd.read_csv(TRANSPORTS_CSV)\n",
        "\n",
        "print(f\"‚úÖ Shipments: {len(shipments)} rows\")\n",
        "print(f\"‚úÖ Lanes: {len(lanes)} rows\")\n",
        "print(f\"‚úÖ Warehouses: {len(warehouses)} rows\")\n",
        "print(f\"‚úÖ Transports: {len(transports)} rows\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 2Ô∏è‚É£ Rename overlapping columns before merging\n",
        "# --------------------------\n",
        "lanes = lanes.rename(columns={\n",
        "    \"distance_km\": \"lane_distance_km\",\n",
        "    \"delay_rate\": \"lane_delay_rate\",\n",
        "    \"avg_lead_time_days\": \"lane_lead_time_days\"\n",
        "})\n",
        "\n",
        "warehouses = warehouses.rename(columns={\n",
        "    \"avg_procurement_cost_per_sku\": \"wh_procurement_cost\",\n",
        "    \"service_score\": \"wh_service_score\"\n",
        "})\n",
        "\n",
        "transports = transports.rename(columns={\n",
        "    \"base_cost_per_km\": \"tr_base_cost_per_km\",\n",
        "    \"reliability\": \"tr_reliability\",\n",
        "    \"co2_kg_per_km\": \"tr_co2_kg_per_km\"\n",
        "})\n",
        "\n",
        "# --------------------------\n",
        "# 3Ô∏è‚É£ Merge all datasets\n",
        "# --------------------------\n",
        "df = shipments.merge(lanes, how=\"left\",\n",
        "                     on=[\"warehouse_id\", \"store_id\", \"transport_id\"])\n",
        "df = df.merge(warehouses, how=\"left\", on=\"warehouse_id\")\n",
        "df = df.merge(transports, how=\"left\", on=\"transport_id\")\n",
        "\n",
        "# Verify uniqueness\n",
        "if df.columns.duplicated().any():\n",
        "    print(\"‚ö†Ô∏è Warning: Duplicate columns still exist, removing duplicates...\")\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "print(f\"üßæ Final columns ({len(df.columns)}): {list(df.columns)}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 4Ô∏è‚É£ Feature setup\n",
        "# --------------------------\n",
        "df = df.rename(columns={\n",
        "    \"quantity_units\": \"qty\",\n",
        "    \"unit_landed_cost\": \"target_unit_cost\"\n",
        "})\n",
        "\n",
        "feature_columns_numeric = [\n",
        "    \"qty\", \"lane_distance_km\", \"lane_delay_rate\", \"lane_lead_time_days\",\n",
        "    \"wh_procurement_cost\", \"wh_service_score\",\n",
        "    \"tr_base_cost_per_km\", \"tr_reliability\", \"tr_co2_kg_per_km\"\n",
        "]\n",
        "cat_columns = [\"warehouse_id\", \"store_id\", \"transport_id\", \"sku\"]\n",
        "\n",
        "# Ensure numeric columns exist\n",
        "for c in feature_columns_numeric:\n",
        "    if c not in df.columns:\n",
        "        df[c] = 0.0\n",
        "\n",
        "# Drop missing targets\n",
        "df = df.dropna(subset=[\"target_unit_cost\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"üß† Features: {len(feature_columns_numeric)} numeric, {len(cat_columns)} categorical\")\n",
        "print(f\"üéØ Target: 'target_unit_cost' ({len(df)} samples)\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 5Ô∏è‚É£ Train/test split\n",
        "# --------------------------\n",
        "X = df[feature_columns_numeric + cat_columns]\n",
        "y = df[\"target_unit_cost\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
        ")\n",
        "print(f\"üìä Train: {len(X_train)}, Test: {len(X_test)}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 6Ô∏è‚É£ Preprocessing pipeline\n",
        "# --------------------------\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, feature_columns_numeric),\n",
        "        (\"cat\", categorical_transformer, cat_columns),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 7Ô∏è‚É£ Model pipeline\n",
        "# --------------------------\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=300, max_depth=4, learning_rate=0.05, random_state=RANDOM_SEED\n",
        ")\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", gbr)\n",
        "])\n",
        "\n",
        "print(\"üèóÔ∏è Training compatibility model... (this may take ~20‚Äì30s)\\n\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# --------------------------\n",
        "# 8Ô∏è‚É£ Evaluate\n",
        "# --------------------------\n",
        "y_pred = pipeline.predict(X_test)\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"‚úÖ Validation RMSE: {rmse:.3f}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 9Ô∏è‚É£ Feature importances\n",
        "# --------------------------\n",
        "try:\n",
        "    model = pipeline.named_steps[\"model\"]\n",
        "    pre = pipeline.named_steps[\"preprocessor\"]\n",
        "    num_feats = feature_columns_numeric\n",
        "    ohe = pre.named_transformers_[\"cat\"]\n",
        "    cat_feat_names = list(ohe.get_feature_names_out(cat_columns))\n",
        "    feature_names = num_feats + cat_feat_names\n",
        "    importances = model.feature_importances_\n",
        "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "    imp_df = imp_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    print(\"üìà Top 15 feature importances:\")\n",
        "    print(imp_df.head(15), \"\\n\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Could not compute feature importances:\", e)\n",
        "\n",
        "# --------------------------\n",
        "# üîü Save model\n",
        "# --------------------------\n",
        "joblib.dump(pipeline, OUTPUT_MODEL_PATH)\n",
        "print(f\"üíæ Saved trained compatibility pipeline to: {OUTPUT_MODEL_PATH}\\n\")\n",
        "\n",
        "# Sanity check\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "sample = X_test.iloc[[sample_idx]]\n",
        "pred = pipeline.predict(sample)[0]\n",
        "print(\"üîç Sample prediction check:\")\n",
        "print(sample)\n",
        "print(f\"Predicted unit cost: {pred:.2f}\\n\")\n",
        "\n",
        "print(\"üéâ Compatibility model training completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCeGBEC0E3q_",
        "outputId": "3e11146e-10e9-4f2e-a837-939ac8a59694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting compatibility model training...\n",
            "\n",
            "üìÇ Loading datasets from: /content/synthetic_supply_chain_data\n",
            "‚úÖ Shipments: 2500 rows\n",
            "‚úÖ Lanes: 240 rows\n",
            "‚úÖ Warehouses: 6 rows\n",
            "‚úÖ Transports: 4 rows\n",
            "\n",
            "üßæ Final columns (22): ['sku', 'warehouse_id', 'store_id', 'transport_id', 'quantity_units', 'unit_landed_cost', 'lead_time_days', 'delayed', 'co2_emission_kg', 'lane_distance_km', 'lane_delay_rate', 'lane_lead_time_days', 'region', 'capacity_units', 'operating_cost_per_day', 'wh_procurement_cost', 'wh_service_score', 'mode', 'tr_base_cost_per_km', 'avg_speed_kmph', 'tr_reliability', 'tr_co2_kg_per_km']\n",
            "\n",
            "üß† Features: 9 numeric, 4 categorical\n",
            "üéØ Target: 'target_unit_cost' (2500 samples)\n",
            "\n",
            "üìä Train: 2000, Test: 500\n",
            "\n",
            "üèóÔ∏è Training compatibility model... (this may take ~20‚Äì30s)\n",
            "\n",
            "‚úÖ Validation RMSE: 2.946\n",
            "\n",
            "üìà Top 15 feature importances:\n",
            "                feature  importance\n",
            "0   wh_procurement_cost    0.356587\n",
            "1                   qty    0.288769\n",
            "2      lane_distance_km    0.092409\n",
            "3     transport_id_T003    0.063734\n",
            "4      tr_co2_kg_per_km    0.060681\n",
            "5        tr_reliability    0.060306\n",
            "6   tr_base_cost_per_km    0.055851\n",
            "7      wh_service_score    0.005604\n",
            "8       lane_delay_rate    0.003076\n",
            "9         store_id_S009    0.002558\n",
            "10  lane_lead_time_days    0.002050\n",
            "11        store_id_S001    0.001940\n",
            "12    warehouse_id_W004    0.001524\n",
            "13    warehouse_id_W005    0.000681\n",
            "14           sku_SKU010    0.000654 \n",
            "\n",
            "üíæ Saved trained compatibility pipeline to: /content/compat_model.joblib\n",
            "\n",
            "üîç Sample prediction check:\n",
            "     qty  lane_distance_km  lane_delay_rate  lane_lead_time_days  \\\n",
            "940   97               730             0.01                  0.8   \n",
            "\n",
            "     wh_procurement_cost  wh_service_score  tr_base_cost_per_km  \\\n",
            "940                24.87               0.8                  2.5   \n",
            "\n",
            "     tr_reliability  tr_co2_kg_per_km warehouse_id store_id transport_id  \\\n",
            "940            0.99               1.2         W003     S003         T003   \n",
            "\n",
            "        sku  \n",
            "940  SKU011  \n",
            "Predicted unit cost: 41.03\n",
            "\n",
            "üéâ Compatibility model training completed successfully!\n"
          ]
        }
      ]
    }
  ]
}