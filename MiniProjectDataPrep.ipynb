{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IvJcFC2hcex",
        "outputId": "06dcfd33-7573-4111-cd51-bd93798a3fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3674238175.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_processed = df_mvp.groupby('Product ID', group_keys=False).apply(create_ts_features)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processed Data with Features & Target ---\n",
            "| Date                | Product ID   | Units Sold   | Inventory Level   | Lag_Sales_D-7   | Lag_Inventory_D-1   | Target_Sales_D+7   | Day_of_Week   |\n",
            "|:--------------------|:-------------|:-------------|:------------------|:----------------|:--------------------|:-------------------|:--------------|\n",
            "| 2022-01-05 00:00:00 | P0001        | 147          | 238               | 127             | 85                  | 941                | 2             |\n",
            "| 2022-01-06 00:00:00 | P0001        | 101          | 185               | 104             | 238                 | 1163               | 3             |\n",
            "| 2022-01-07 00:00:00 | P0001        | 97           | 227               | 81              | 185                 | 1018               | 4             |\n",
            "| 2022-01-08 00:00:00 | P0001        | 341          | 349               | 67              | 227                 | 1088               | 5             |\n",
            "| 2022-01-09 00:00:00 | P0001        | 155          | 361               | 66              | 349                 | 1036               | 6             |\n",
            "| 2022-01-10 00:00:00 | P0001        | 42           | 117               | 3               | 361                 | 771                | 0             |\n",
            "| 2022-01-10 00:00:00 | P0001        | 280          | 310               | 58              | 117                 | 655                | 0             |\n",
            "| 2022-01-13 00:00:00 | P0001        | 2            | 218               | 147             | 310                 | 676                | 3             |\n",
            "| 2022-01-13 00:00:00 | P0001        | 171          | 206               | 101             | 218                 | 572                | 3             |\n",
            "| 2022-01-14 00:00:00 | P0001        | 45           | 81                | 97              | 206                 | 667                | 4             |\n",
            "\n",
            "--- Processed Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 14351 entries, 400 to 72699\n",
            "Data columns (total 19 columns):\n",
            " #   Column              Non-Null Count  Dtype         \n",
            "---  ------              --------------  -----         \n",
            " 0   Date                14351 non-null  datetime64[ns]\n",
            " 1   Store ID            14351 non-null  object        \n",
            " 2   Product ID          14351 non-null  object        \n",
            " 3   Category            14351 non-null  object        \n",
            " 4   Region              14351 non-null  object        \n",
            " 5   Inventory Level     14351 non-null  int64         \n",
            " 6   Units Sold          14351 non-null  int64         \n",
            " 7   Units Ordered       14351 non-null  int64         \n",
            " 8   Demand Forecast     14351 non-null  float64       \n",
            " 9   Price               14351 non-null  float64       \n",
            " 10  Discount            14351 non-null  int64         \n",
            " 11  Weather Condition   14351 non-null  object        \n",
            " 12  Holiday/Promotion   14351 non-null  int64         \n",
            " 13  Competitor Pricing  14351 non-null  float64       \n",
            " 14  Seasonality         14351 non-null  object        \n",
            " 15  Lag_Sales_D-7       14351 non-null  float64       \n",
            " 16  Lag_Inventory_D-1   14351 non-null  float64       \n",
            " 17  Target_Sales_D+7    14351 non-null  float64       \n",
            " 18  Day_of_Week         14351 non-null  int32         \n",
            "dtypes: datetime64[ns](1), float64(6), int32(1), int64(5), object(6)\n",
            "memory usage: 2.1+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"retail_store_inventory.csv\")\n",
        "\n",
        "# 1. Convert Date to Datetime objects\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# 2. Filter for MVP: Select a single category and sort the data\n",
        "# Sorting is CRITICAL for time series feature creation (lags and targets)\n",
        "MVP_CATEGORY = 'Groceries'\n",
        "df_mvp = df[df['Category'] == MVP_CATEGORY].sort_values(['Product ID', 'Date']).copy()\n",
        "\n",
        "# 3. Feature & Target Engineering (Grouped by Product ID)\n",
        "# We must apply these time-based calculations *per product* to avoid data leakage\n",
        "# (i.e., we don't want Product P0001's lag to use P0002's data)\n",
        "\n",
        "def create_ts_features(group):\n",
        "    # --- LAG FEATURES (The Past) ---\n",
        "    # Lag 7: Sales from 7 days ago\n",
        "    group['Lag_Sales_D-7'] = group['Units Sold'].shift(7)\n",
        "    # Lag 1: Inventory from 1 day ago (the stock the model \"knows\" about today)\n",
        "    group['Lag_Inventory_D-1'] = group['Inventory Level'].shift(1)\n",
        "\n",
        "    # --- TARGET VARIABLE (The Future) ---\n",
        "    # Target: Total Sales for the next 7 days (This is what the model must predict)\n",
        "    # The rolling sum is calculated over the next 7 periods (6 periods after the current day + the current day)\n",
        "    # We use .shift(-6) to align the 7-day future sum with the current row's date\n",
        "    group['Target_Sales_D+7'] = group['Units Sold'].rolling(\n",
        "        window=7, closed='left'\n",
        "    ).sum().shift(-6)\n",
        "\n",
        "    # We must also create a simple time feature (Day of Week)\n",
        "    group['Day_of_Week'] = group['Date'].dt.dayofweek\n",
        "    return group\n",
        "\n",
        "df_processed = df_mvp.groupby('Product ID', group_keys=False).apply(create_ts_features)\n",
        "\n",
        "# Clean up rows that now have missing values due to lagging/shifting\n",
        "# (These are the first 7 rows of each time series that cannot have a full history/future)\n",
        "df_processed.dropna(subset=['Target_Sales_D+7', 'Lag_Sales_D-7', 'Lag_Inventory_D-1'], inplace=True)\n",
        "\n",
        "# Display the resulting head to see the new features\n",
        "print(\"--- Processed Data with Features & Target ---\")\n",
        "print(df_processed[['Date', 'Product ID', 'Units Sold', 'Inventory Level', 'Lag_Sales_D-7', 'Lag_Inventory_D-1', 'Target_Sales_D+7', 'Day_of_Week']].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Check final shape and data types\n",
        "print(\"\\n--- Processed Data Info ---\")\n",
        "print(df_processed.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed.to_csv('ProcessedRetailDataset.csv', index=False)"
      ],
      "metadata": {
        "id": "__NgGkLKjP4m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"retail_store_inventory.csv\")\n",
        "\n",
        "# 1. Convert Date to Datetime objects\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# 2. Select Multiple Categories for Scaling Up\n",
        "SELECTED_CATEGORIES = ['Groceries', 'Toys', 'Clothing']\n",
        "df_combined = df[df['Category'].isin(SELECTED_CATEGORIES)].sort_values(['Product ID', 'Date']).copy()\n",
        "\n",
        "# 3. Feature & Target Engineering (Adding D-1 and D-2 Lags)\n",
        "def create_ts_features_v2(group):\n",
        "    # --- LAG FEATURES (The Past & Present) ---\n",
        "    # Primary Lags for immediate momentum (NEW)\n",
        "    group['Lag_Sales_D-1'] = group['Units Sold'].shift(1)\n",
        "    group['Lag_Sales_D-2'] = group['Units Sold'].shift(2)\n",
        "\n",
        "    # Existing Lags\n",
        "    group['Lag_Sales_D-7'] = group['Units Sold'].shift(7)\n",
        "    group['Lag_Inventory_D-1'] = group['Inventory Level'].shift(1)\n",
        "\n",
        "    # --- ROLLING STATISTICAL FEATURE ---\n",
        "    group['Rolling_Mean_7D'] = group['Units Sold'].shift(1).rolling(window=7).mean()\n",
        "\n",
        "    # --- TARGET VARIABLE (The Future) ---\n",
        "    # Target: Total Sales for the next 7 days (D+1 to D+7)\n",
        "    group['Target_Sales_D+7'] = group['Units Sold'].rolling(\n",
        "        window=7, closed='left'\n",
        "    ).sum().shift(-6)\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply the new feature creation function\n",
        "df_processed_combined = df_combined.groupby('Product ID', group_keys=False).apply(create_ts_features_v2)\n",
        "\n",
        "# 4. Create Categorical ID Feature\n",
        "le = LabelEncoder()\n",
        "df_processed_combined['Category_ID'] = le.fit_transform(df_processed_combined['Category'])\n",
        "\n",
        "# 5. Clean up rows that now have missing values due to lagging/rolling\n",
        "df_processed_combined.dropna(subset=[\n",
        "    'Target_Sales_D+7',\n",
        "    'Lag_Sales_D-7',\n",
        "    'Lag_Sales_D-1',\n",
        "    'Lag_Sales_D-2',\n",
        "    'Lag_Inventory_D-1',\n",
        "    'Rolling_Mean_7D'\n",
        "], inplace=True)\n",
        "\n",
        "# 6. Save the processed DataFrame to a new CSV file\n",
        "output_file_name = \"Combined_3_Categories_Processed_Data_V2.csv\"\n",
        "df_processed_combined.to_csv(output_file_name, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqXYHyJA76D2",
        "outputId": "f4659826-755f-4a34-fe0d-0614e4fe853b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3054268330.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_processed_combined = df_combined.groupby('Product ID', group_keys=False).apply(create_ts_features_v2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the original dataset\n",
        "df = pd.read_csv(\"retail_store_inventory.csv\")\n",
        "\n",
        "# 1. Convert Date to Datetime objects\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# 2. Prepare ALL Categories and sort the data\n",
        "# Sorting by Product ID and Date is crucial for correct lag calculations\n",
        "df_all = df.sort_values(['Product ID', 'Date']).copy()\n",
        "\n",
        "# 3. Feature & Target Engineering Function\n",
        "def create_ts_features_v3(group):\n",
        "    \"\"\"Calculates lag, rolling mean, and future target sales for a single product group.\"\"\"\n",
        "\n",
        "    # --- LAG FEATURES (The Past & Present) ---\n",
        "    # Lag D-1 and D-2 for immediate momentum (to minimize phase lag)\n",
        "    group['Lag_Sales_D-1'] = group['Units Sold'].shift(1)\n",
        "    group['Lag_Sales_D-2'] = group['Units Sold'].shift(2)\n",
        "\n",
        "    # Weekly lag for seasonality\n",
        "    group['Lag_Sales_D-7'] = group['Units Sold'].shift(7)\n",
        "\n",
        "    # Inventory from previous day (The Present)\n",
        "    group['Lag_Inventory_D-1'] = group['Inventory Level'].shift(1)\n",
        "\n",
        "    # --- ROLLING STATISTICAL FEATURE ---\n",
        "    # 7-day Rolling Mean of Sales (shifted by 1 to prevent data leakage)\n",
        "    group['Rolling_Mean_7D'] = group['Units Sold'].shift(1).rolling(window=7).mean()\n",
        "\n",
        "    # --- TARGET VARIABLE (The Future) ---\n",
        "    # Target: Total Sales for the next 7 days (Label is placed on the date the prediction is made)\n",
        "    # Uses 'closed=left' so the sum includes sales from the current row + next 6 rows.\n",
        "    group['Target_Sales_D+7'] = group['Units Sold'].rolling(\n",
        "        window=7, closed='left'\n",
        "    ).sum().shift(-6)\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply the feature creation function across ALL product IDs\n",
        "df_processed_all = df_all.groupby('Product ID', group_keys=False).apply(create_ts_features_v3)\n",
        "\n",
        "# 4. Create Categorical ID Feature\n",
        "# This is used for the Embedding layer in the final neural network\n",
        "le = LabelEncoder()\n",
        "df_processed_all['Category_ID'] = le.fit_transform(df_processed_all['Category'])\n",
        "\n",
        "# 5. Clean up rows with NaN values resulting from the lags (first 7-8 rows of each time series)\n",
        "df_processed_all.dropna(subset=[\n",
        "    'Target_Sales_D+7',\n",
        "    'Lag_Sales_D-7',\n",
        "    'Lag_Sales_D-1',\n",
        "    'Lag_Sales_D-2',\n",
        "    'Lag_Inventory_D-1',\n",
        "    'Rolling_Mean_7D'\n",
        "], inplace=True)\n",
        "\n",
        "# 6. Save the final processed DataFrame\n",
        "output_file_name = \"Combined_5_Categories_Processed_Data_V3.csv\"\n",
        "df_processed_all.to_csv(output_file_name, index=False)\n",
        "\n",
        "print(f\"The final, complete dataset has been generated and saved as '{output_file_name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eQSCsf0B5PJ",
        "outputId": "b943b52c-4003-4f1e-8a80-63f72de6d5d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2555516678.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_processed_all = df_all.groupby('Product ID', group_keys=False).apply(create_ts_features_v3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The final, complete dataset has been generated and saved as 'Combined_5_Categories_Processed_Data_V3.csv'.\n"
          ]
        }
      ]
    }
  ]
}