{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCeGBEC0E3q_",
        "outputId": "3e11146e-10e9-4f2e-a837-939ac8a59694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Base directory set to: c:\\Users\\sujal\\Projects\\mini\\backend\n",
            "üìÅ Working directory changed to: c:\\Users\\sujal\\Projects\\mini\\backend\n",
            "üìÇ Loading datasets from: c:\\Users\\sujal\\Projects\\mini\\backend\\data\n",
            "‚úÖ Shipments: 500 rows\n",
            "‚úÖ Lanes: 50 rows\n",
            "‚úÖ Warehouses: 5 rows\n",
            "‚úÖ Transports: 3 rows\n",
            "\n",
            "üß† Features ready: 9 numeric, 4 categorical\n",
            "üéØ Samples for training: 500\n",
            "\n",
            "üèóÔ∏è Training compatibility model...\n",
            "\n",
            "‚úÖ Validation RMSE: 15.703\n",
            "\n",
            "üìà Top 10 feature importances:\n",
            "               feature  importance\n",
            "0                  qty    0.229806\n",
            "1     lane_distance_km    0.140111\n",
            "2  lane_lead_time_days    0.063807\n",
            "3  wh_procurement_cost    0.036882\n",
            "4      warehouse_id_W2    0.032206\n",
            "5          store_id_S2    0.031749\n",
            "6             sku_SKU2    0.023243\n",
            "7      lane_delay_rate    0.021071\n",
            "8          store_id_S6    0.020344\n",
            "9  tr_base_cost_per_km    0.019889 \n",
            "\n",
            "üíæ Model saved to: c:\\Users\\sujal\\Projects\\mini\\backend\\models\\compat_model.joblib\n",
            "\n",
            "üîç Sample prediction check:\n",
            "     qty  lane_distance_km  lane_delay_rate  lane_lead_time_days  \\\n",
            "431  352             500.0             0.05                  5.0   \n",
            "\n",
            "     wh_procurement_cost  wh_service_score  tr_base_cost_per_km  \\\n",
            "431                 65.0              0.72                 5.72   \n",
            "\n",
            "     tr_reliability  tr_co2_kg_per_km warehouse_id store_id transport_id  \\\n",
            "431            0.92              0.12           W4       S9           T1   \n",
            "\n",
            "       sku  \n",
            "431  SKU13  \n",
            "Predicted unit cost: 38.91\n",
            "\n",
            "üéâ Compatibility model training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "from math import sqrt\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ SET ROBUST PATHS\n",
        "# ============================================================\n",
        "\n",
        "CWD = os.getcwd()\n",
        "\n",
        "def find_backend_dir(start_path):\n",
        "    \"\"\"Search upward for a folder containing 'data' and 'models'.\"\"\"\n",
        "    path = start_path\n",
        "    while True:\n",
        "        data_path = os.path.join(path, \"data\")\n",
        "        models_path = os.path.join(path, \"models\")\n",
        "        backend_path = os.path.join(path, \"backend\")\n",
        "\n",
        "        # Case 1: inside backend\n",
        "        if os.path.isdir(data_path) and os.path.isdir(models_path):\n",
        "            return path\n",
        "\n",
        "        # Case 2: one level above backend\n",
        "        if os.path.isdir(backend_path) and \\\n",
        "           os.path.isdir(os.path.join(backend_path, \"data\")) and \\\n",
        "           os.path.isdir(os.path.join(backend_path, \"models\")):\n",
        "            return backend_path\n",
        "\n",
        "        parent = os.path.dirname(path)\n",
        "        if parent == path:\n",
        "            return None\n",
        "        path = parent\n",
        "\n",
        "BASE_DIR = find_backend_dir(CWD)\n",
        "\n",
        "if BASE_DIR is None:\n",
        "    print(f\"‚ùå ERROR: Could not find 'data' or 'models' folders in or above {CWD}\")\n",
        "    raise FileNotFoundError(\"Could not auto-locate data/models directories.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Base directory set to: {BASE_DIR}\")\n",
        "    os.chdir(BASE_DIR)\n",
        "    print(f\"üìÅ Working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ USER SETTINGS\n",
        "# ============================================================\n",
        "\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "SHIPMENTS_CSV = os.path.join(DATA_DIR, \"shipments.csv\")\n",
        "LANES_CSV = os.path.join(DATA_DIR, \"lanes.csv\")\n",
        "WAREHOUSES_CSV = os.path.join(DATA_DIR, \"warehouses.csv\")\n",
        "TRANSPORTS_CSV = os.path.join(DATA_DIR, \"transports.csv\")\n",
        "\n",
        "OUTPUT_MODEL_PATH = os.path.join(MODELS_DIR, \"compat_model.joblib\")\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ GENERATE SYNTHETIC DATA (if missing or empty)\n",
        "# ============================================================\n",
        "\n",
        "def generate_mock_data():\n",
        "    np.random.seed(42)\n",
        "\n",
        "    print(\"üß™ Generating synthetic mock data...\")\n",
        "\n",
        "    warehouses = pd.DataFrame({\n",
        "        \"warehouse_id\": [f\"W{i}\" for i in range(1, 6)],\n",
        "        \"location\": np.random.choice([\"Mumbai\", \"Delhi\", \"Chennai\", \"Bangalore\", \"Hyderabad\"], 5, replace=False),\n",
        "        \"avg_procurement_cost_per_sku\": np.random.uniform(50, 200, 5).round(2),\n",
        "        \"service_score\": np.random.uniform(0.7, 1.0, 5).round(2)\n",
        "    })\n",
        "\n",
        "    transports = pd.DataFrame({\n",
        "        \"transport_id\": [f\"T{i}\" for i in range(1, 4)],\n",
        "        \"mode\": [\"Road\", \"Rail\", \"Air\"],\n",
        "        \"base_cost_per_km\": np.random.uniform(1.5, 6.0, 3).round(2),\n",
        "        \"reliability\": np.random.uniform(0.8, 1.0, 3).round(2),\n",
        "        \"co2_kg_per_km\": np.random.uniform(0.1, 1.0, 3).round(2)\n",
        "    })\n",
        "\n",
        "    stores = [f\"S{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # ‚úÖ Create ALL combinations of warehouse‚Äìstore‚Äìtransport to avoid NaNs\n",
        "    lanes_data = []\n",
        "    for w in warehouses[\"warehouse_id\"]:\n",
        "        for s in stores:\n",
        "            for t in transports[\"transport_id\"]:\n",
        "                lanes_data.append({\n",
        "                    \"warehouse_id\": w,\n",
        "                    \"store_id\": s,\n",
        "                    \"transport_id\": t,\n",
        "                    \"distance_km\": np.random.uniform(100, 1500),\n",
        "                    \"delay_rate\": np.random.uniform(0.01, 0.15),\n",
        "                    \"avg_lead_time_days\": np.random.uniform(1, 10)\n",
        "                })\n",
        "    lanes = pd.DataFrame(lanes_data)\n",
        "\n",
        "    skus = [f\"SKU{i}\" for i in range(1, 21)]\n",
        "    shipments_data = []\n",
        "    for _ in range(500):  # 500 shipment records\n",
        "        wh = np.random.choice(warehouses[\"warehouse_id\"])\n",
        "        st = np.random.choice(stores)\n",
        "        tr = np.random.choice(transports[\"transport_id\"])\n",
        "        sku = np.random.choice(skus)\n",
        "        qty = np.random.randint(10, 500)\n",
        "        base_cost = np.random.uniform(5, 50)\n",
        "\n",
        "        # Always guaranteed to find lane match now\n",
        "        distance = lanes.loc[\n",
        "            (lanes[\"warehouse_id\"] == wh) &\n",
        "            (lanes[\"store_id\"] == st) &\n",
        "            (lanes[\"transport_id\"] == tr),\n",
        "            \"distance_km\"\n",
        "        ].iloc[0]\n",
        "\n",
        "        unit_landed_cost = (base_cost + 0.02 * distance + np.random.uniform(-2, 5))\n",
        "        shipments_data.append({\n",
        "            \"warehouse_id\": wh,\n",
        "            \"store_id\": st,\n",
        "            \"transport_id\": tr,\n",
        "            \"sku\": sku,\n",
        "            \"quantity_units\": qty,\n",
        "            \"unit_landed_cost\": round(unit_landed_cost, 2)\n",
        "        })\n",
        "\n",
        "    shipments = pd.DataFrame(shipments_data)\n",
        "\n",
        "    warehouses.to_csv(WAREHOUSES_CSV, index=False)\n",
        "    transports.to_csv(TRANSPORTS_CSV, index=False)\n",
        "    lanes.to_csv(LANES_CSV, index=False)\n",
        "    shipments.to_csv(SHIPMENTS_CSV, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Synthetic datasets created at {DATA_DIR}\")\n",
        "    print(f\" Warehouses: {warehouses.shape}, Transports: {transports.shape}, Lanes: {lanes.shape}, Shipments: {shipments.shape}\\n\")\n",
        "\n",
        "# Create data if missing or empty\n",
        "if not os.path.exists(SHIPMENTS_CSV) or os.path.getsize(SHIPMENTS_CSV) == 0:\n",
        "    generate_mock_data()\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ LOAD DATA\n",
        "# ============================================================\n",
        "\n",
        "print(\"üìÇ Loading datasets from:\", DATA_DIR)\n",
        "shipments = pd.read_csv(SHIPMENTS_CSV)\n",
        "lanes = pd.read_csv(LANES_CSV)\n",
        "warehouses = pd.read_csv(WAREHOUSES_CSV)\n",
        "transports = pd.read_csv(TRANSPORTS_CSV)\n",
        "\n",
        "print(f\"‚úÖ Shipments: {len(shipments)} rows\")\n",
        "print(f\"‚úÖ Lanes: {len(lanes)} rows\")\n",
        "print(f\"‚úÖ Warehouses: {len(warehouses)} rows\")\n",
        "print(f\"‚úÖ Transports: {len(transports)} rows\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ RENAME + MERGE DATASETS\n",
        "# ============================================================\n",
        "\n",
        "lanes = lanes.rename(columns={\n",
        "    \"distance_km\": \"lane_distance_km\",\n",
        "    \"delay_rate\": \"lane_delay_rate\",\n",
        "    \"avg_lead_time_days\": \"lane_lead_time_days\"\n",
        "})\n",
        "\n",
        "warehouses = warehouses.rename(columns={\n",
        "    \"avg_procurement_cost_per_sku\": \"wh_procurement_cost\",\n",
        "    \"service_score\": \"wh_service_score\"\n",
        "})\n",
        "\n",
        "transports = transports.rename(columns={\n",
        "    \"base_cost_per_km\": \"tr_base_cost_per_km\",\n",
        "    \"reliability\": \"tr_reliability\",\n",
        "    \"co2_kg_per_km\": \"tr_co2_kg_per_km\"\n",
        "})\n",
        "\n",
        "df = shipments.merge(lanes, how=\"left\", on=[\"warehouse_id\", \"store_id\", \"transport_id\"])\n",
        "df = df.merge(warehouses, how=\"left\", on=\"warehouse_id\")\n",
        "df = df.merge(transports, how=\"left\", on=\"transport_id\")\n",
        "\n",
        "# Remove duplicates if any\n",
        "if df.columns.duplicated().any():\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ CLEAN AND PREPARE FEATURES\n",
        "# ============================================================\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"quantity_units\": \"qty\",\n",
        "    \"unit_landed_cost\": \"target_unit_cost\"\n",
        "})\n",
        "\n",
        "feature_columns_numeric = [\n",
        "    \"qty\", \"lane_distance_km\", \"lane_delay_rate\", \"lane_lead_time_days\",\n",
        "    \"wh_procurement_cost\", \"wh_service_score\",\n",
        "    \"tr_base_cost_per_km\", \"tr_reliability\", \"tr_co2_kg_per_km\"\n",
        "]\n",
        "cat_columns = [\"warehouse_id\", \"store_id\", \"transport_id\", \"sku\"]\n",
        "\n",
        "# Fill NaNs (safety step)\n",
        "df = df.fillna({\n",
        "    \"lane_distance_km\": 500,\n",
        "    \"lane_delay_rate\": 0.05,\n",
        "    \"lane_lead_time_days\": 5,\n",
        "    \"wh_procurement_cost\": 100,\n",
        "    \"wh_service_score\": 0.85,\n",
        "    \"tr_base_cost_per_km\": 3.0,\n",
        "    \"tr_reliability\": 0.9,\n",
        "    \"tr_co2_kg_per_km\": 0.5,\n",
        "})\n",
        "for c in cat_columns:\n",
        "    df[c] = df[c].fillna(\"UNKNOWN\")\n",
        "\n",
        "# Drop missing targets\n",
        "df = df.dropna(subset=[\"target_unit_cost\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"üß† Features ready: {len(feature_columns_numeric)} numeric, {len(cat_columns)} categorical\")\n",
        "print(f\"üéØ Samples for training: {len(df)}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ TRAIN/TEST SPLIT + PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "X = df[feature_columns_numeric + cat_columns]\n",
        "y = df[\"target_unit_cost\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, feature_columns_numeric),\n",
        "        (\"cat\", categorical_transformer, cat_columns),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=300, max_depth=4, learning_rate=0.05, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", gbr)\n",
        "])\n",
        "\n",
        "print(\"üèóÔ∏è Training compatibility model...\\n\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================\n",
        "# 8Ô∏è‚É£ EVALUATE\n",
        "# ============================================================\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"‚úÖ Validation RMSE: {rmse:.3f}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 9Ô∏è‚É£ FEATURE IMPORTANCES\n",
        "# ============================================================\n",
        "\n",
        "try:\n",
        "    model = pipeline.named_steps[\"model\"]\n",
        "    pre = pipeline.named_steps[\"preprocessor\"]\n",
        "    ohe = pre.named_transformers_[\"cat\"]\n",
        "    cat_feat_names = list(ohe.get_feature_names_out(cat_columns))\n",
        "    feature_names = feature_columns_numeric + cat_feat_names\n",
        "    importances = model.feature_importances_\n",
        "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "    imp_df = imp_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    print(\"üìà Top 10 feature importances:\")\n",
        "    print(imp_df.head(10), \"\\n\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Could not compute feature importances:\", e)\n",
        "\n",
        "# ============================================================\n",
        "# üîü SAVE MODEL + SAMPLE PREDICTION\n",
        "# ============================================================\n",
        "\n",
        "joblib.dump(pipeline, OUTPUT_MODEL_PATH)\n",
        "print(f\"üíæ Model saved to: {OUTPUT_MODEL_PATH}\\n\")\n",
        "\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "sample = X_test.iloc[[sample_idx]]\n",
        "pred = pipeline.predict(sample)[0]\n",
        "print(\"üîç Sample prediction check:\")\n",
        "print(sample)\n",
        "print(f\"Predicted unit cost: {pred:.2f}\\n\")\n",
        "\n",
        "print(\"üéâ Compatibility model training completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
